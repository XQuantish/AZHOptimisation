{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "algo_nn.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "gcDAqxWyYRJA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simple neural network"
      ]
    },
    {
      "metadata": {
        "id": "aVNZT0PMcpLW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare colab"
      ]
    },
    {
      "metadata": {
        "id": "kowQUqGCclJn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9a12c6e7-6eec-4a8b-d565-36a40b83ccbd"
      },
      "cell_type": "code",
      "source": [
        "!pip install pydot\n",
        "!pip install GraphViz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (1.2.4)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.2.2)\n",
            "Requirement already satisfied: GraphViz in /usr/local/lib/python3.6/dist-packages (0.10.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uz9eNGqrc3LN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "065aaca2-1cb3-4e53-ca9b-9663b3b25b67"
      },
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tlog.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hQlunCY2YRJG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Config"
      ]
    },
    {
      "metadata": {
        "id": "n4LaXYUVYRJH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### CONFIG ###\n",
        "thefile = 'prepared-20181001_prefix-pNN_input_2tag_Sig_BKGs.pickle'\n",
        "### END ###\n",
        "\n",
        "do3tag = False\n",
        "doTrainNetwork = True\n",
        "\n",
        "layer_size = 1\n",
        "dropout = None\n",
        "nepochs = 50\n",
        "batch_size = 128\n",
        "modeltag = 'nn_l{0}e{1}b{2}'.format(layer_size,nepochs,batch_size)\n",
        "\n",
        "netfile = \"model_{0}.h5\".format(modeltag)\n",
        "graphfile = \"graph_{0}.png\".format(modeltag)\n",
        "file_performance = \"perf_{0}.pickle\".format(modeltag)\n",
        "file_minmax_scaler = \"minmax_scaler.pickle\"\n",
        "file_quantile_scaler = \"quantile_scaler.pickle\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AtLoEFMKYRJQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import"
      ]
    },
    {
      "metadata": {
        "id": "inX12mt8YRJS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53ff5cdb-3683-49f1-b7da-8d20fb9652e2"
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import re\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "import pickle\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import QuantileTransformer, MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.regularizers import l1_l2\n",
        "from keras.optimizers import SGD\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "g-fmf7NKYRJg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Assistant functions"
      ]
    },
    {
      "metadata": {
        "id": "mtJBc1RfYRJi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "#############################################################\n",
        "def setup_custom_logger(name):\n",
        "    formatter = logging.Formatter(fmt='%(asctime)s %(levelname)-8s %(message)s',\n",
        "                                  datefmt='%Y-%m-%d %H:%M:%S')\n",
        "    handler = logging.FileHandler('log.txt', mode='w')\n",
        "    handler.setFormatter(formatter)\n",
        "    screen_handler = logging.StreamHandler(stream=sys.stdout)\n",
        "    screen_handler.setFormatter(formatter)\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    logger.addHandler(handler)\n",
        "    logger.addHandler(screen_handler)\n",
        "    return logger\n",
        "\n",
        "#############################################################\n",
        "def get_model(n_invars, n_hidden_nodes, regularization=None, dropout=None):\n",
        "    if not isinstance(n_hidden_nodes, list):\n",
        "        n_hidden_nodes = [n_hidden_nodes]\n",
        "\n",
        "    x = Input(shape=(n_invars,))\n",
        "    d = x\n",
        "    for n in n_hidden_nodes:\n",
        "        d = Dense(n, activation=\"relu\", kernel_regularizer=regularization)(d)\n",
        "        if dropout:\n",
        "            d = Dropout(dropout)(d)\n",
        "    y = Dense(1, activation=\"sigmoid\")(d)\n",
        "    return Model(x, y)\n",
        "##########################################################\n",
        "def preprocess(X, quantile_trsf=None, minmax_trsf=None):\n",
        "    if quantile_trsf is None and minmax_trsf is None:\n",
        "        quantile_trsf = QuantileTransformer()\n",
        "        minmax_trsf = MinMaxScaler()\n",
        "        quantile_trsf.fit(X[:, :-1])\n",
        "        minmax_trsf.fit(X[:, -1][:, np.newaxis])\n",
        "    elif quantile_trsf is not None and minmax_trsf is not None:\n",
        "        pass\n",
        "    else:\n",
        "        return None\n",
        "    X_trsf_no_mass = quantile_trsf.transform(X[:, :-1])\n",
        "    X_trsf_mass = minmax_trsf.transform(X[:, -1][:, np.newaxis])\n",
        "\n",
        "    return quantile_trsf, minmax_trsf, np.hstack([X_trsf_no_mass, X_trsf_mass])\n",
        "##########################################################\n",
        "\n",
        "\n",
        "#############################################################\n",
        "import numpy as np\n",
        "\n",
        "# map mA,mH -> DISD\n",
        "dict_mAmH_dsid = {\n",
        "  'ggF_llbb' : {\n",
        "    (230,130) : 306939,\n",
        "    (250,130) : 306940,\n",
        "    (230,150) : 306941,\n",
        "    (300,130) : 306942,\n",
        "    (300,150) : 306943,\n",
        "    (300,200) : 306944,\n",
        "    (350,250) : 306945,\n",
        "    (400,130) : 306946,\n",
        "    (400,200) : 306948,\n",
        "    (400,250) : 344587,\n",
        "    (500,130) : 306952,\n",
        "    (500,200) : 306955,\n",
        "    (500,300) : 306958,\n",
        "    (500,350) : 308468,\n",
        "    (500,400) : 306959,\n",
        "    (600,130) : 306962,\n",
        "    (600,300) : 306966,\n",
        "    (600,400) : 344588,\n",
        "    (600,450) : 308469,\n",
        "    (600,500) : 306967,\n",
        "    (700,130) : 306968,\n",
        "    (700,200) : 306970,\n",
        "    (700,300) : 306972,\n",
        "    (700,400) : 306973,\n",
        "    (700,500) : 306974,\n",
        "    (700,600) : 308568,\n",
        "    (800,130) : 308569,\n",
        "    (800,300) : 308570,\n",
        "    (800,500) : 344589,\n",
        "    (800,700) : 308571,\n",
        "  },\n",
        "}\n",
        "\n",
        "mH_min = 130\n",
        "mH_max = 700\n",
        "mA_min = 230\n",
        "mA_max = 800\n",
        "\n",
        "# mAmH -> DSID list\n",
        "#############################################################\n",
        "# mAmH_list is a list of mA,mH pair\n",
        "# prod: ggF_llbb, bbA_llbb ...\n",
        "def mAmH2disd( mAmH_list, prod ):\n",
        "  return [ dict_mAmH_dsid[prod][(mA,mH)] for mA,mH in mAmH_list ]\n",
        "\n",
        "# random sampling of background mA and mH\n",
        "#############################################################\n",
        "# nm: mA or mH\n",
        "# the whole df\n",
        "def sample_bkg_mass(nm, df, method=0, seed=None):\n",
        "  n_bkg = (~df.IsSignal).sum()\n",
        "  m_bkg = None\n",
        "  if seed:\n",
        "    np.random.seed(seed)\n",
        "  if method==0:\n",
        "    m_bkg = np.random.choice(df[nm][~df.IsSignal],size=n_bkg)\n",
        "  else:\n",
        "    #m_bkg = np.random.uniform(np.min(m[isSig]),np.max(m[isSig]),size=n_bkg)\n",
        "    if nm == 'mA':\n",
        "      m_bkg = np.random.uniform( mA_min, mH_min, size=n_bkg)\n",
        "    if nm == 'mH':\n",
        "      m_bkg = np.random.uniform( mH_min, mA_max, size=n_bkg)\n",
        "  df.loc[~df.IsSignal, nm] = m_bkg\n",
        "\n",
        "# choose signal samples\n",
        "#############################################################\n",
        "# mAmH_list is a list of mA,mH pair\n",
        "# prod: ggF_llbb, bbA_llbb ...\n",
        "def choose_signal_samples( df, mAmH_list, prod ):\n",
        "  print('choose_signal_samples: signal tot # =',df.IsSignal.sum())\n",
        "  dsid_list = mAmH2disd( mAmH_list, prod )\n",
        "  df_chosen = df[ ((df.IsSignal) & (df.label.isin(dsid_list)) ) | (~df.IsSignal) ]\n",
        "  print('choose_signal_samples: signal tot chosen to use # =',df_chosen.IsSignal.sum() )\n",
        "  return df_chosen\n",
        "\n",
        "# list input variables\n",
        "#############################################################\n",
        "def input_variables( do3tag, df ):\n",
        "  inputs = None\n",
        "  if do3tag:\n",
        "    inputs = df[[\"lep0pt\",\"lep0eta\",\"lep0phi\",\n",
        "             \"lep1pt\",\"lep1eta\",\"lep1phi\",\n",
        "             \"jet0pt\",\"jet0eta\",\"jet0phi\",\n",
        "             \"jet1pt\",\"jet1eta\",\"jet1phi\",\n",
        "             \"jet2pt\",\"jet2eta\",\"jet2phi\",\n",
        "             \"mA\",\"mH\"]].values\n",
        "  else:\n",
        "    inputs = df[[\"lep0pt\",\"lep0eta\",\"lep0phi\",\n",
        "             \"lep1pt\",\"lep1eta\",\"lep1phi\",\n",
        "             \"jet0pt\",\"jet0eta\",\"jet0phi\",\n",
        "             \"jet1pt\",\"jet1eta\",\"jet1phi\",\n",
        "             \"mA\",\"mH\"]].values\n",
        "  return inputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YLoWTC6SYmgO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## System"
      ]
    },
    {
      "metadata": {
        "id": "EItvFvbpYwR3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# system\n",
        "logger = setup_custom_logger('logger_algo_nn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KcESbOVNZHWN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load in data"
      ]
    },
    {
      "metadata": {
        "id": "9mQECRksY04q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "632fc1a2-f831-4bb0-8e57-655c5b055577"
      },
      "cell_type": "code",
      "source": [
        "logger.info('1. Load in dataset ...')\n",
        "\n",
        "if os.path.isfile('/content/gdrive/My Drive/AZHOptimisation/{0}'.format(thefile)):\n",
        "  pass\n",
        "else:\n",
        "  logger.info(' + mount google drive ...')\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-10-30 19:36:54 INFO     1. Load in dataset ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zLhZPi6eZhp2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "640646d0-1d12-495b-b5bb-f550c06c9997"
      },
      "cell_type": "code",
      "source": [
        "logger.info(' + load in dataframe ...')\n",
        "df = None\n",
        "with open('/content/gdrive/My Drive/AZHOptimisation/{0}'.format(thefile), 'rb') as f:\n",
        "  df = pickle.load( f )\n",
        "  logger.info('load in done')\n",
        "print(df.head())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-10-30 19:36:55 INFO      + load in dataframe ...\n",
            "2018-10-30 19:37:18 INFO     load in done\n",
            "   label    weight     lep0pt   lep0eta   lep0phi     lep1pt   lep1eta  \\\n",
            "0    0.0  1.127729  67.842850  0.202458  0.776767  24.886770 -0.710250   \n",
            "1    0.0  0.981583  53.870251  1.400823 -0.405050  32.279022  0.701465   \n",
            "2    0.0  1.276938  43.058762  0.653615 -2.372478  31.318354 -1.273499   \n",
            "3    0.0 -1.263140  40.866436 -0.733332 -0.106703  31.655827 -1.992335   \n",
            "4    0.0 -2.159261  82.256416  0.107891  1.170636  24.290808  0.053033   \n",
            "\n",
            "    lep1phi     jet0pt   jet0eta    ...     jet2eta  jet2phi        MET  \\\n",
            "0 -2.265613  82.877945  1.044228    ...         0.0      0.0  20.968199   \n",
            "1  2.193279  50.149277  0.604722    ...         0.0      0.0  30.534142   \n",
            "2  2.797295  88.430099 -1.511340    ...         0.0      0.0  31.228664   \n",
            "3  2.209157  49.726997  0.705296    ...         0.0      0.0   9.700199   \n",
            "4 -2.224541  60.990715  2.497605    ...         0.0      0.0   5.708796   \n",
            "\n",
            "     METSig  NBJet     NEvent  EventNumber   mA   mH  IsSignal  \n",
            "0  1.367891    2.0   648250.0            0  0.0  0.0     False  \n",
            "1  2.311807    2.0  1477018.0            1  0.0  0.0     False  \n",
            "2  2.308603    2.0   102096.0            2  0.0  0.0     False  \n",
            "3  0.619642    2.0  2303024.0            3  0.0  0.0     False  \n",
            "4  0.333419    2.0   497839.0            4  0.0  0.0     False  \n",
            "\n",
            "[5 rows x 25 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CN-saXwbb0m0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare training testing dataset"
      ]
    },
    {
      "metadata": {
        "id": "-FlfBtaEay2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "af843a38-2ca5-49e6-caa7-9f662d18421d"
      },
      "cell_type": "code",
      "source": [
        "logger.info('2. Prepare training testing datasets')\n",
        "logger.info(' + choose signal samples (may not all mA,mH)')\n",
        "df = choose_signal_samples( df, [ [400,200], [700,200], [700,500] ], 'ggF_llbb' )\n",
        "logger.info(' + generate random mA,mH for bkg')\n",
        "sample_bkg_mass('mA', df, 1) # mH\n",
        "sample_bkg_mass('mH', df, 1) # mA\n",
        "logger.info(' + connect input variables depending on #bjets')\n",
        "X = input_variables( do3tag, df )\n",
        "logger.info(' + prepare target Y with type int')\n",
        "Y = df.IsSignal.values.astype(int)\n",
        "logger.info(' + shuffle all chosen entries')\n",
        "X, Y = shuffle(X,Y)\n",
        "logger.info(' + split sample for training 2/3, testing 1/3')\n",
        "N = len(X)\n",
        "separ = int(2*N/3)\n",
        "X_train = X[:separ]\n",
        "Y_train  = Y[:separ]\n",
        "X_test = X[separ:]\n",
        "Y_test = Y[separ:]\n",
        "logger.info('Training sample: {0}; Tesiting sample: {1}'.format( len(X_train), len(X_test) ))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-10-30 19:37:19 INFO     2. Prepare training testing datasets\n",
            "2018-10-30 19:37:19 INFO      + choose signal samples (may not all mA,mH)\n",
            "choose_signal_samples: signal tot # = 310511\n",
            "choose_signal_samples: signal tot chosen to use # = 34282\n",
            "2018-10-30 19:37:23 INFO      + generate random mA,mH for bkg\n",
            "2018-10-30 19:37:25 INFO      + connect input variables depending on #bjets\n",
            "2018-10-30 19:37:26 INFO      + prepare target Y with type int\n",
            "2018-10-30 19:37:26 INFO      + shuffle all chosen entries\n",
            "2018-10-30 19:37:30 INFO      + split sample for training 2/3, testing 1/3\n",
            "2018-10-30 19:37:30 INFO     Training sample: 11004202; Tesiting sample: 5502102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S-radKs3cET9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train and test NN"
      ]
    },
    {
      "metadata": {
        "id": "atTIJBSSb-nG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "699e100e-f0b7-4e35-89cc-2b60564e874d"
      },
      "cell_type": "code",
      "source": [
        "model = None\n",
        "minmax_trsf = None\n",
        "quantile_trsf = None\n",
        "fitres = []\n",
        "\n",
        "if doTrainNetwork:\n",
        "    logger.info('3. Train NN with {0} layers, {1} epochs, {2} batch size, {3} dropout. model tag is {4}'.format(layer_size,nepochs,batch_size,dropout,modeltag) )\n",
        "\n",
        "    best_loss = 1e9\n",
        "    min_delta = 0.0001\n",
        "    patience = 25\n",
        "    epochs_not_improved = 0\n",
        "    # Reduce LR on plateau\n",
        "    lr_best_loss = 1e9\n",
        "    lr_patience = 10\n",
        "    lr_factor = 0.5\n",
        "    lr_epochs_not_improved = 0\n",
        "\n",
        "    _, n_invars = X.shape\n",
        "    model = get_model(n_invars, layer_size, dropout=dropout)\n",
        "    sgd = SGD(lr=0.1, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd,loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
        "    #plot_model(model, to_file=graphfile, show_shapes=True)\n",
        "    # Preprocessing\n",
        "    logger.info(' + preprocessing ...')\n",
        "    quantile_trsf, minmax_trsf, X_train_trsf = preprocess(X_train)\n",
        "    _,_, X_test_trsf = preprocess(X_test, quantile_trsf, minmax_trsf)\n",
        "\n",
        "    logger.info(' + training ...')\n",
        "    logger.info(\"{:>10}{:>10}\".format(\"Epoch\", \"Loss\"))\n",
        "    _res = None\n",
        "    for epoch in range(nepochs):\n",
        "        # sample_bkg_mass(X_train_trsf[:, -1], Y_train) # <--------- TEST IT\n",
        "        _res = model.fit(X_train_trsf, Y_train, batch_size=batch_size, epochs=1, validation_data=(X_test_trsf,Y_test), verbose=0)\n",
        "        fitres.append(_res.history)\n",
        "        loss_epoch = _res.history['loss'][0] # as nepoch is 1\n",
        "\n",
        "        logger.info(\"{:>10.0f}{:>10.5f}\".format(epoch, loss_epoch))\n",
        "        # Reduce LR on plateau\n",
        "        if loss_epoch + min_delta < lr_best_loss:\n",
        "            lr_best_loss = loss_epoch\n",
        "            lr_epochs_not_improved = 0\n",
        "        else:\n",
        "            lr_epochs_not_improved += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if loss_epoch + min_delta < best_loss:\n",
        "            best_loss = loss_epoch\n",
        "            epochs_not_improved = 0\n",
        "        else:\n",
        "            epochs_not_improved += 1\n",
        "\n",
        "        # Reduce LR on plateau\n",
        "        if lr_epochs_not_improved > lr_patience:\n",
        "            current_lr = K.get_value(model.optimizer.lr)\n",
        "            logger.info(\"Current LR: {}\".format(current_lr))\n",
        "            logger.info(\"Reducing LR to: {}\".format(lr_factor * current_lr))\n",
        "            K.set_value(model.optimizer.lr, lr_factor * current_lr)\n",
        "            lr_epochs_not_improved = 0\n",
        "\n",
        "        # Early stopping\n",
        "        if epochs_not_improved > patience:\n",
        "            logger.info(\"Early stopping after epoch {}\".format(epoch))\n",
        "            break\n",
        "    # loop ends here\n",
        "    # now save the network\n",
        "    logger.info(' + save history of performance to file {0}'.format(file_performance))\n",
        "    with open( file_performance, 'wb') as savehandle:\n",
        "        pickle.dump(fitres, savehandle, protocol=3)\n",
        "    logger.info(' + dump model and transformers to files {0} {1} {2}'.format(netfile,file_minmax_scaler,file_quantile_scaler))\n",
        "    joblib.dump(minmax_trsf, file_minmax_scaler)\n",
        "    joblib.dump(quantile_trsf, file_quantile_scaler)\n",
        "    model.save(netfile)\n",
        "\n",
        "    # print perfomance\n",
        "    logger.info(' + print performance')\n",
        "    logger.info('loss:{0}'.format(fitres[-1]['loss']))\n",
        "    logger.info('loss validation:{0}'.format(fitres[-1]['val_loss']))\n",
        "    logger.info('accuracy:{0}'.format(fitres[-1]['acc']))\n",
        "    logger.info('accuracy validation:{0}'.format(fitres[-1]['val_acc']))\n",
        "\n",
        "else:\n",
        "    logger.info( \"3. No traininng , directly application, read in the nn model and transformer files\")\n",
        "    logger.info(' + read in files {0} {1} {2}'.format(netfile,file_minmax_scaler,file_quantile_scaler))\n",
        "    model = load_model(netfile)\n",
        "    quantile_scaler = joblib.load(file_quantile_scaler)\n",
        "    minmax_scaler = joblib.load(file_minmax_scaler)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-10-30 19:38:14 INFO     3. Train NN with 1 layers, 50 epochs, 128 batch size, None dropout. model tag is nn_l1e50b128\n",
            "2018-10-30 19:38:14 INFO      + preprocessing ...\n",
            "2018-10-30 19:39:40 INFO      + training ...\n",
            "2018-10-30 19:39:40 INFO          Epoch      Loss\n",
            "2018-10-30 19:46:45 INFO              0   0.00618\n",
            "2018-10-30 19:53:49 INFO              1   0.00448\n",
            "2018-10-30 20:00:51 INFO              2   0.00398\n",
            "2018-10-30 20:07:53 INFO              3   0.00371\n",
            "2018-10-30 20:14:54 INFO              4   0.00346\n",
            "2018-10-30 20:21:56 INFO              5   0.00330\n",
            "2018-10-30 20:28:59 INFO              6   0.00317\n",
            "2018-10-30 20:36:02 INFO              7   0.00303\n",
            "2018-10-30 20:43:03 INFO              8   0.00291\n",
            "2018-10-30 20:50:06 INFO              9   0.00282\n",
            "2018-10-30 20:57:10 INFO             10   0.00276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b4YeHgZzcVwJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}